{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Libs-and-Functions\" data-toc-modified-id=\"Libs-and-Functions-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Libs and Functions</a></span></li><li><span><a href=\"#Data-Understanding\" data-toc-modified-id=\"Data-Understanding-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Data Understanding</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this notebook is to propose an analytical view of e-commerce relationship in Brazil. For this we will first go trough an exploratory data analysis using graphical tools to create self explanatory plots for better understanding what is behind braziian online purchasing. Finally we will look at customers reviews and implement **_Sentimental Analysis_** to make a text classification using **_Natural Language Process_** tools.\n",
    "\n",
    "Let's get to work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libs and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:09:28.808899Z",
     "start_time": "2019-08-25T03:09:27.246126Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import missingno as msno\n",
    "import re\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "import string\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import urlextract\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "import itertools\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:43:50.798401Z",
     "start_time": "2019-08-25T03:43:50.689289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Defining functions\n",
    "def format_spines(ax, right_border=True):\n",
    "    \"\"\"\n",
    "    This function is responsible for format axis from graphs\n",
    "    \n",
    "    Input:\n",
    "        ax: matplotlib axis\n",
    "        right_border: boolean flag for plot the right border of graphs\n",
    "    \n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    ax.spines['bottom'].set_color('#CCCCCC')\n",
    "    ax.spines['left'].set_color('#CCCCCC')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    if right_border:\n",
    "        ax.spines['right'].set_color('#CCCCCC')\n",
    "    else:\n",
    "        ax.spines['right'].set_color('#FFFFFF')\n",
    "    ax.patch.set_facecolor('#FFFFFF')\n",
    "    \n",
    "def bar_plot(x, y, df, ax, colors='Blues_d', hue=False, value=False):\n",
    "    \"\"\"\n",
    "    This function plots, and customize a bar chart\n",
    "\n",
    "    Input:\n",
    "        x: feature to be plotted on x axis\n",
    "        y: feature to be plotted on y axis\n",
    "        df: DataFrame object with features used on x and y\n",
    "        colors: standard palette is \"Blues_d\"\n",
    "        hue: separation value, standard is False\n",
    "        value: flag for defining if the data labels on bars will be the value (True) or percentual (False)\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Counting numerical feature (x or y)\n",
    "    try:\n",
    "        ncount = sum(df[y])\n",
    "    except:\n",
    "        ncount = sum(df[x])\n",
    "    \n",
    "    # Plotting\n",
    "    if hue != False:\n",
    "        ax = sns.barplot(x=x, y=y, data=df, palette=colors, hue=hue, ax=ax, ci=None)\n",
    "    else:\n",
    "        ax = sns.barplot(x=x, y=y, data=df, palette=colors, ax=ax, ci=None)\n",
    "\n",
    "    # Customizing data labels (values of percents)\n",
    "    for p in ax.patches:\n",
    "        xp=p.get_bbox().get_points()[:,0]\n",
    "        yp=p.get_bbox().get_points()[1,1]\n",
    "        if value:\n",
    "            ax.annotate('{:.2f}k'.format(yp/1000), (xp.mean(), yp), \n",
    "                    ha='center', va='bottom')\n",
    "        else:\n",
    "            ax.annotate('{:.1f}%'.format(100.*yp/ncount), (xp.mean(), yp), \n",
    "                    ha='center', va='bottom')\n",
    "            \n",
    "def add_series_working_days(series_name, df, date_col1, date_col2):\n",
    "    \"\"\"\n",
    "    This function is used for calculating working days between two dates\n",
    "    as an additional column on a DataFrame\n",
    "    \n",
    "    Input:\n",
    "        series_names: name of the new series created with working days calculated\n",
    "        df: DataFrame object with the dates\n",
    "        date_col1: date column 1\n",
    "        date_col2: date column 2\n",
    "        \n",
    "    Returns:\n",
    "        df_return: DataFrame with working days columns\n",
    "    \"\"\"\n",
    "    # Creating a list with the difference between dates\n",
    "    time_list = []\n",
    "    idx = 0\n",
    "    second_date_series = df[date_col2].values.astype('datetime64[D]')\n",
    "    for date in df[date_col1].values.astype('datetime64[D]'):\n",
    "        second_date = second_date_series[idx]\n",
    "        try:\n",
    "            workdays = np.busday_count(date, second_date)\n",
    "        except:\n",
    "            workdays = np.NaN\n",
    "        time_list.append(workdays)\n",
    "        idx += 1\n",
    "    \n",
    "    # Adding column in a DataFrame object\n",
    "    df_return = df.copy()\n",
    "    df_return[series_name] = pd.Series(time_list)\n",
    "    df_return.dropna(inplace=True)\n",
    "    \n",
    "    return df_return\n",
    "\n",
    "def communicate_params(freight, deliv, estimative):\n",
    "    \"\"\"\n",
    "    Additional function create for helping on graphical exploration (state dashboard)\n",
    "    \n",
    "    Input:\n",
    "        freight: freight value to be plotted as a text\n",
    "        deliv: delivery time to be plotted as a text\n",
    "        estimative: differente between delivery time and estimative time to be plotted as a text\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"    \n",
    "    # Plotting mean freight value\n",
    "    ax1.text(0.23, 0.6, f'R${frete:.2f}', fontsize=65)\n",
    "    ax1.text(0.22, 0.45, 'is the average freight', fontsize=12)\n",
    "    ax1.text(0.24, 0.4, 'for online purchasing', fontsize=12)\n",
    "    ax1.axis('off')\n",
    "    \n",
    "    # Plotting average delivery time\n",
    "    axs[0, 1].text(0.5, 0.12, \n",
    "                   str(f'Average Delivery Time\\n{int(deliv)} working days.'),\n",
    "                   fontsize=13, ha='center')\n",
    "    axs[0, 1].axis('off')\n",
    "    \n",
    "    # Plotting difference between delivery and estimative\n",
    "    if mean_diff_estimative > 0:\n",
    "        inf = 'before'\n",
    "    else:\n",
    "        inf = 'after'\n",
    "    text_diff_estimative = f'Normally, deliveries \\nhappen {int(estimative)} working days \\\n",
    "\\n{inf} from estimated time.'\n",
    "    axs[0, 2].text(0.5, 0.10, \n",
    "                   str(text_diff_estimative), fontsize=13, ha='center')\n",
    "    axs[0, 2].axis('off')\n",
    "    \n",
    "def plot_param(df, col, title, xlim, n_row, n_col, y='customer_state', div_xlim=0, \n",
    "               one_axis=False, xlabel=[], ylabel='Estado'):\n",
    "    \"\"\"\n",
    "    This function is used for plotting a comparative study on 5 better and 5 worst according to the topic\n",
    "    \n",
    "    Input:\n",
    "        df: DataFrame object with the data\n",
    "        col: column to be studied\n",
    "        title: title\n",
    "        xlim: xlim\n",
    "        n_row: line index where the graph will be plotted\n",
    "        n_col: column index where the graph will be plotted\n",
    "    \"\"\"\n",
    "    # Axis definition\n",
    "    if one_axis:\n",
    "        ax_top = axs[n_col]\n",
    "        ax_last = axs[n_col+1]\n",
    "    else:\n",
    "        ax_top = axs[n_row, n_col]\n",
    "        ax_last = axs[n_row+1, n_col]\n",
    "    \n",
    "    # First step: Top 5\n",
    "    df.sort_values(by=col, ascending=False, inplace=True)\n",
    "    top5 = df.iloc[:5, :]\n",
    "    sns.barplot(x=col, y=y, data=top5, ci=None, palette='Blues_d', ax=ax_top)\n",
    "    format_spines(ax_top, right_border=False)\n",
    "    ax_top.set_title(title)\n",
    "    ax_top.set_xlim(0, xlim)\n",
    "    ax_top.set_xlabel(xlabel)\n",
    "    if n_col > 0:\n",
    "        ax_top.set_ylabel('')\n",
    "    else:\n",
    "        ax_top.set_ylabel(ylabel)\n",
    "    \n",
    "    # Second step: Last 5\n",
    "    last5 = df.iloc[-5:, :]\n",
    "    sns.barplot(x=col, y=y, data=last5, ci=None, palette='Blues_d', ax=ax_last)\n",
    "    format_spines(ax_last, right_border=False)\n",
    "    ax_last.set_title(title.replace('Maior', 'Menor'))\n",
    "    if div_xlim > 0:\n",
    "        ax_last.set_xlim(0, xlim/div_xlim)\n",
    "    else:\n",
    "        ax_last.set_xlim(0, xlim)\n",
    "    ax_last.set_xlabel(xlabel)\n",
    "    if n_col > 0:\n",
    "        ax_last.set_ylabel('')\n",
    "    else:\n",
    "        ax_last.set_ylabel(ylabel)\n",
    "        \n",
    "def donut_plot(col, ax, df, labels, text='', flag_ruido = 0,\n",
    "               colors=['navy', 'lightsteelblue', 'lightgreen', 'crimson', '']):\n",
    "    \"\"\"\n",
    "    This function plots a customized donut plot\n",
    "    \n",
    "    Input:\n",
    "        col: coluna a ser analisada e plotada no gráfico de rosca\n",
    "        ax: matplotlib axis\n",
    "        df: DataFrame with data\n",
    "        labels: list of labels to be plotted\n",
    "        text: text to be plotted on the center of the donut\n",
    "        flag_ruido: thie parameter indicates the number of labels to be filtered from plot\n",
    "        colors: list of colors (4 colors from default)\n",
    "    \"\"\"\n",
    "    flag_ruido = flag_ruido * -1\n",
    "    if flag_ruido < 0:\n",
    "        sizes = df[col].value_counts().values[:flag_ruido]\n",
    "        labels = labels[:flag_ruido]\n",
    "    else:\n",
    "        sizes = df[col].value_counts().values\n",
    "    center_circle = plt.Circle((0,0), 0.80, color='white')\n",
    "    ax.pie(sizes, labels=labels, colors=colors, autopct='%1.2f%%')\n",
    "    ax.add_artist(center_circle)\n",
    "    kwargs = dict(size=20, fontweight='bold', va='center')\n",
    "    ax.text(0, 0, text, ha='center', **kwargs)\n",
    "    \n",
    "def text_process(c):\n",
    "    \"\"\"\n",
    "    Function responsible for removing punctuation and stopwords from reviews\n",
    "    \n",
    "    Input:\n",
    "        c: customer review\n",
    "    \n",
    "    Output:\n",
    "        reviews without punctuation and stopwords\n",
    "    \"\"\"\n",
    "    # Remove punctuation\n",
    "    nopunc = [char for char in c if char not in string.punctuation]\n",
    "\n",
    "    # Join string again\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    return [word.lower() for word in nopunc.split() if word.lower() not in stopwords.words('portuguese')]\n",
    "\n",
    "def stem_processing(c):\n",
    "    \"\"\"\n",
    "    Function repsonsible for apply stemming on reviews\n",
    "    \n",
    "    Input:\n",
    "        c: customer review\n",
    "        \n",
    "    Output:\n",
    "        review after stemming\n",
    "    \"\"\"\n",
    "    \n",
    "    stemmer = RSLPStemmer()\n",
    "    return list(map(lambda x: stemmer.stem(x), [word for word in c.split()]))\n",
    "\n",
    "def create_dataset():\n",
    "    \"\"\"\n",
    "    Function responsible for creating an empty DataFrame object with some of classification metrics\n",
    "    \n",
    "    Returns:\n",
    "        empty DataFrame object\n",
    "    \"\"\"\n",
    "    attributes = ['acc', 'prec', 'rec', 'f1', 'total_time']\n",
    "    model_performance = pd.DataFrame({})\n",
    "    for col in attributes:\n",
    "        model_performance[col] = []\n",
    "        \n",
    "    return model_performance\n",
    "\n",
    "def model_analysis(model, X, y, X_test, y_test, df_performance, cv=5, train=True):\n",
    "    \"\"\"\n",
    "    Function responsible for evaluate a classification model and save the results on a DataFrame object\n",
    "    \n",
    "    Input:\n",
    "        model: model to be used on evaluation\n",
    "        X, y, X_test, y_test: train and test data (with target labels)\n",
    "        df_performance: empty DataFrame (generated by create_dataset() function)\n",
    "        cv: cross validation k folds\n",
    "\n",
    "    Returns:\n",
    "        a DataFrame object with classification metrics selected\n",
    "    \"\"\"\n",
    "    # Accuracy, precision, recall and f1_score on training set using cv\n",
    "    t0_cv = time.time()\n",
    "    acc = cross_val_score(model, X, y, cv=cv, scoring='accuracy').mean()\n",
    "    prec = cross_val_score(model, X, y, cv=cv, scoring='precision').mean()\n",
    "    rec = cross_val_score(model, X, y, cv=cv, scoring='recall').mean()\n",
    "    f1 = cross_val_score(model, X, y, cv=cv, scoring='f1').mean()\n",
    "    # Time spent on cross_validation prediction\n",
    "    t1_cv = time.time()\n",
    "    delta_time_cv = t1_cv-t0_cv\n",
    "    \n",
    "    # Evaluation using the test set\n",
    "    t0_test = time.time()\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    acc_test = accuracy_score(y_test, y_pred_test)\n",
    "    prec_test = precision_score(y_test, y_pred_test)\n",
    "    rec_test = recall_score(y_test, y_pred_test)\n",
    "    f1_test = f1_score(y_test, y_pred_test)\n",
    "    y_scores_test = model.predict_proba(X_test)[:, 1]\n",
    "    # Time spent on test prediction\n",
    "    t1_test = time.time()\n",
    "    delta_time_test = t1_test-t0_test\n",
    "\n",
    "    # Saving on dataframe\n",
    "    performances = {}\n",
    "    performances['acc'] = round(acc, 4)\n",
    "    performances['prec'] = round(prec, 4)\n",
    "    performances['rec'] = round(rec, 4)\n",
    "    performances['f1'] = round(f1, 4)\n",
    "    performances['total_time'] = round(delta_time_cv, 3)        \n",
    "    df_performance = df_performance.append(performances, ignore_index=True)\n",
    "    \n",
    "    test_performances = {}\n",
    "    test_performances['acc'] = round(acc_test, 4)\n",
    "    test_performances['prec'] = round(prec_test, 4)\n",
    "    test_performances['rec'] = round(rec_test, 4)\n",
    "    test_performances['f1'] = round(f1_test, 4)\n",
    "    test_performances['total_time'] = round(delta_time_test, 3)        \n",
    "    df_performance = df_performance.append(test_performances, ignore_index=True)\n",
    "    \n",
    "    model_name = model.__class__.__name__\n",
    "    df_performance.index = [model_name+' cv', model_name+' test']\n",
    "    \n",
    "    return df_performance\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a customized confusion matrix\n",
    "    \n",
    "    Input:\n",
    "        cm: confusion matrix generated from sklearn's method confusion_matrix(set, predictions)\n",
    "        classes: target labels to be plotted\n",
    "        title: title\n",
    "        cmap: matrix color\n",
    "    Output:\n",
    "        None\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    # Plot configuration\n",
    "    thresh = cm.max() / 1.2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j]),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the following datasets:\n",
    "\n",
    "    olist_customers_dataset.csv\n",
    "    olist_geolocation_dataset.csv\n",
    "    olist_orders_dataset.csv\n",
    "    olist_order_items_dataset.csv\n",
    "    olist_order_payments_dataset.csv\n",
    "    olist_order_reviews_dataset.csv\n",
    "    olist_products_dataset.csv\n",
    "    olist_sellers_dataset.csv\n",
    "    product_category_name_translation.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/brazilian_e-commerce_schema.png\" alt=\"Dataset Schema\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the data model. The relationship between datasets is provided by foreign keys in each one of the sets. It requires specific methods like join, concat and others for joining the sets and making an analysis on them. For this first moment, let's just read the files and see a little bit more of each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:47:28.525242Z",
     "start_time": "2019-08-25T03:47:26.453623Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reading files\n",
    "olist_customer = pd.read_csv('olist_customers_dataset.csv')\n",
    "olist_geolocation = pd.read_csv('olist_geolocation_dataset.csv')\n",
    "olist_orders = pd.read_csv('olist_orders_dataset.csv')\n",
    "olist_order_items = pd.read_csv('olist_order_items_dataset.csv')\n",
    "olist_order_payments = pd.read_csv('olist_order_payments_dataset.csv')\n",
    "olist_order_reviews = pd.read_csv('olist_order_reviews_dataset.csv')\n",
    "olist_products = pd.read_csv('olist_products_dataset.csv')\n",
    "olist_sellers = pd.read_csv('olist_sellers_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:47:44.620824Z",
     "start_time": "2019-08-25T03:47:44.585134Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>order_status</th>\n",
       "      <th>order_purchase_timestamp</th>\n",
       "      <th>order_approved_at</th>\n",
       "      <th>order_delivered_carrier_date</th>\n",
       "      <th>order_delivered_customer_date</th>\n",
       "      <th>order_estimated_delivery_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e481f51cbdc54678b7cc49136f2d6af7</td>\n",
       "      <td>9ef432eb6251297304e76186b10a928d</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-10-02 10:56:33</td>\n",
       "      <td>2017-10-02 11:07:15</td>\n",
       "      <td>2017-10-04 19:55:00</td>\n",
       "      <td>2017-10-10 21:25:13</td>\n",
       "      <td>2017-10-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53cdb2fc8bc7dce0b6741e2150273451</td>\n",
       "      <td>b0830fb4747a6c6d20dea0b8c802d7ef</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-07-24 20:41:37</td>\n",
       "      <td>2018-07-26 03:24:27</td>\n",
       "      <td>2018-07-26 14:31:00</td>\n",
       "      <td>2018-08-07 15:27:45</td>\n",
       "      <td>2018-08-13 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>47770eb9100c2d0c44946d9cf07ec65d</td>\n",
       "      <td>41ce2a54c0b03bf3443c3d931a367089</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-08-08 08:38:49</td>\n",
       "      <td>2018-08-08 08:55:23</td>\n",
       "      <td>2018-08-08 13:50:00</td>\n",
       "      <td>2018-08-17 18:06:29</td>\n",
       "      <td>2018-09-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>949d5b44dbf5de918fe9c16f97b45f8a</td>\n",
       "      <td>f88197465ea7920adcdbec7375364d82</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2017-11-18 19:28:06</td>\n",
       "      <td>2017-11-18 19:45:59</td>\n",
       "      <td>2017-11-22 13:39:59</td>\n",
       "      <td>2017-12-02 00:28:42</td>\n",
       "      <td>2017-12-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ad21c59c0840e6cb83a9ceb5573f8159</td>\n",
       "      <td>8ab97904e6daea8866dbdbc4fb7aad2c</td>\n",
       "      <td>delivered</td>\n",
       "      <td>2018-02-13 21:18:39</td>\n",
       "      <td>2018-02-13 22:20:29</td>\n",
       "      <td>2018-02-14 19:46:34</td>\n",
       "      <td>2018-02-16 18:17:02</td>\n",
       "      <td>2018-02-26 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           order_id                       customer_id  \\\n",
       "0  e481f51cbdc54678b7cc49136f2d6af7  9ef432eb6251297304e76186b10a928d   \n",
       "1  53cdb2fc8bc7dce0b6741e2150273451  b0830fb4747a6c6d20dea0b8c802d7ef   \n",
       "2  47770eb9100c2d0c44946d9cf07ec65d  41ce2a54c0b03bf3443c3d931a367089   \n",
       "3  949d5b44dbf5de918fe9c16f97b45f8a  f88197465ea7920adcdbec7375364d82   \n",
       "4  ad21c59c0840e6cb83a9ceb5573f8159  8ab97904e6daea8866dbdbc4fb7aad2c   \n",
       "\n",
       "  order_status order_purchase_timestamp    order_approved_at  \\\n",
       "0    delivered      2017-10-02 10:56:33  2017-10-02 11:07:15   \n",
       "1    delivered      2018-07-24 20:41:37  2018-07-26 03:24:27   \n",
       "2    delivered      2018-08-08 08:38:49  2018-08-08 08:55:23   \n",
       "3    delivered      2017-11-18 19:28:06  2017-11-18 19:45:59   \n",
       "4    delivered      2018-02-13 21:18:39  2018-02-13 22:20:29   \n",
       "\n",
       "  order_delivered_carrier_date order_delivered_customer_date  \\\n",
       "0          2017-10-04 19:55:00           2017-10-10 21:25:13   \n",
       "1          2018-07-26 14:31:00           2018-08-07 15:27:45   \n",
       "2          2018-08-08 13:50:00           2018-08-17 18:06:29   \n",
       "3          2017-11-22 13:39:59           2017-12-02 00:28:42   \n",
       "4          2018-02-14 19:46:34           2018-02-16 18:17:02   \n",
       "\n",
       "  order_estimated_delivery_date  \n",
       "0           2017-10-18 00:00:00  \n",
       "1           2018-08-13 00:00:00  \n",
       "2           2018-09-04 00:00:00  \n",
       "3           2017-12-15 00:00:00  \n",
       "4           2018-02-26 00:00:00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of what is on the orders dataset\n",
    "olist_orders.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:48:09.563738Z",
     "start_time": "2019-08-25T03:48:09.539736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers: 5 columns\n",
      "['customer_id', 'customer_unique_id', 'customer_zip_code_prefix', 'customer_city', 'customer_state']\n",
      "\n",
      "Geolocation: 5 columns\n",
      "['geolocation_zip_code_prefix', 'geolocation_lat', 'geolocation_lng', 'geolocation_city', 'geolocation_state']\n",
      "\n",
      "Orders: 8 columns\n",
      "['order_id', 'customer_id', 'order_status', 'order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n",
      "\n",
      "Items: 7 columns\n",
      "['order_id', 'order_item_id', 'product_id', 'seller_id', 'shipping_limit_date', 'price', 'freight_value']\n",
      "\n",
      "Payments: 5 columns\n",
      "['order_id', 'payment_sequential', 'payment_type', 'payment_installments', 'payment_value']\n",
      "\n",
      "Reviews: 7 columns\n",
      "['review_id', 'order_id', 'review_score', 'review_comment_title', 'review_comment_message', 'review_creation_date', 'review_answer_timestamp']\n",
      "\n",
      "Products: 9 columns\n",
      "['product_id', 'product_category_name', 'product_name_lenght', 'product_description_lenght', 'product_photos_qty', 'product_weight_g', 'product_length_cm', 'product_height_cm', 'product_width_cm']\n",
      "\n",
      "Sellers: 4 columns\n",
      "['seller_id', 'seller_zip_code_prefix', 'seller_city', 'seller_state']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# VColumns on each set\n",
    "dataframes = {\n",
    "    'Customers': olist_customer,\n",
    "    'Geolocation': olist_geolocation,\n",
    "    'Orders': olist_orders,\n",
    "    'Items': olist_order_items,\n",
    "    'Payments': olist_order_payments,\n",
    "    'Reviews': olist_order_reviews,\n",
    "    'Products': olist_products,\n",
    "    'Sellers': olist_sellers\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f'{name}: {len(df.columns)} columns')\n",
    "    print(f'{list(df.columns)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-25T03:48:38.507696Z",
     "start_time": "2019-08-25T03:48:38.479351Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Customers: 3 PKs or FKs\n",
      "['customer_id', 'customer_unique_id', 'customer_zip_code_prefix']\n",
      "\n",
      "Geolocation: 1 PKs or FKs\n",
      "['geolocation_zip_code_prefix']\n",
      "\n",
      "Orders: 2 PKs or FKs\n",
      "['order_id', 'customer_id']\n",
      "\n",
      "Items: 4 PKs or FKs\n",
      "['order_id', 'order_item_id', 'product_id', 'seller_id']\n",
      "\n",
      "Payments: 1 PKs or FKs\n",
      "['order_id']\n",
      "\n",
      "Reviews: 2 PKs or FKs\n",
      "['review_id', 'order_id']\n",
      "\n",
      "Products: 1 PKs or FKs\n",
      "['product_id']\n",
      "\n",
      "Sellers: 2 PKs or FKs\n",
      "['seller_id', 'seller_zip_code_prefix']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Foreign keys\n",
    "for name, df in dataframes.items():\n",
    "    chaves = [col for col in df.columns if '_id' in col or 'code' in col]\n",
    "    print(f'{name}: {len(chaves)} PKs or FKs')\n",
    "    print(f'{chaves}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
